annotations_path: /home/nlp/tzufar/data/senticap/annotations
arithmetics_imgs:
- example_images/arithmetics/woman2.jpg
- example_images/arithmetics/king2.jpg
- example_images/arithmetics/man2.jpg
arithmetics_style_imgs:
- '49'
- '50'
- '51'
- '52'
- '53'
arithmetics_weights:
- 1
- -0.5
- 0.5
audio_model_sampling_rate: 48000
audio_path: ~/data/for_audio/child-crying-in-yard-72009.wav
audio_sampling_rate: 24000
audio_temperature: 0.16774988500573
batch_size: 16
beam_size: 5
best_model_name: checkpoints/best_models/senticap/best_text_style_embedding_model_senticap.pth
calc_evaluation: true
calc_fluency: true
caption_img_path: /home/nlp/tzufar/data/imgs/101.jpeg
ce_scale: 4
check_if_cut_score: false
checkpoints_dir: /home/nlp/tzufar/checkpoints
clip_checkpoints: /home/nlp/tzufar/projects/zero-shot-style/zero_shot_style/clip_checkpoints
clip_loss_temperature: 0.01
clip_scale: 7.9012461958729086
cond_text: Image of a
cond_text2: ''
cond_text_dict:
  factual: ''
  negative: ''
  positive: ''
config_file: ./configs/config_audio_laughter.yaml
cuda_idx_num: '0'
cut_cand2clip: false
data_dir: /home/nlp/tzufar/data
data_file: null
data_name: go_emotions
data_type: val
dataset: senticap
debug: false
debug_mac: false
desired_improvement_loss: 0.01
desired_labels:
- negative
desired_min_CLIP_score: 1
desired_min_clip_score: 0.28
desired_min_fluency_score: 0.9
desired_min_style_score: 1
embedding_vectors_std:
  negative: 0.14634644370317826
  positive: 0.031123760342107343
emoji_pretrained_path: /home/nlp/tzufar/projects/torchMoji/model/pytorch_model.bin
emoji_vocab_path: /home/nlp/tzufar/projects/torchMoji/model/vocabulary.json
end_factor: 1.01
end_token: .
epochs: 20
evaluation_metrics:
- CLIPScore
- fluency
- CLAPScore
experiement_global_name: StylizedZeroCap_audio_cry_kids1_laughter_params
experiment_dir: /home/nlp/tzufar/experiments/zero_style_cap/senticap/roberta/StylizedZeroCap_audio_laughter_kids1_sw_f/26_09_2023/55-cqrot3st-revived-sweep-55
experiment_name: cur_time
factual_captions_path: /home/nlp/tzufar/data/source/coco/factual_captions.pkl
finetuned_roberta_config: /home/nlp/tzufar/checkpoints/finetuned_roberta/config.json
finetuned_roberta_model_path: /home/nlp/tzufar/checkpoints/finetuned_roberta/pytorch_model.bin
forbidden_factor: 20
freeze_after_n_epochs: 4
fusion_factor: 0.99
grad_norm_factor: 0.9
heavy_max_num_iterations: 7
hidden_state_to_take_txt_cls: -1
hidden_state_to_take_txt_style_embedding: -2
idx_emoji_style_dict:
  negative:
  - 1
  - 2
  - 3
  - 5
  - 12
  - 22
  - 27
  - 29
  - 32
  - 34
  - 35
  - 37
  - 39
  - 42
  - 43
  - 44
  - 45
  - 46
  - 52
  - 55
  - 56
  - 58
  positive:
  - 0
  - 4
  - 6
  - 7
  - 8
  - 13
  - 15
  - 16
  - 17
  - 18
  - 23
  - 24
  - 36
  - 40
  - 53
  - 60
img_idx_to_start_from: 0
img_name: 0
imgs_dict: /home/nlp/tzufar/data/senticap
imgs_path: /home/nlp/tzufar/data/senticap/images
imitate_text_style: false
inner_batch_size: 1
iterate_until_good_fluency: false
kv_only_first_layer: true
labels_dict_idxs:
  negative: 1
  positive: 0
labels_dict_idxs_roberta:
  negative: 0
  positive: 2
lm_model: gpt-2
load_model: false
loss_scale_src_clip_clip: 2.0e-06
loss_scale_style_clip: 4.013
lr: 1.0e-05
margin: 0.26
max_batch_size_style_cls: 10
max_num_imgs2test: -1
max_num_iterations: 5
max_num_of_imgs: -1
maxlen_emoji_sentence: 30
mean_vec_emb_file: checkpoints/best_models/senticap/senticap_mean_class_embedding.p
median_vec_emb_file: null
model_based_on: bert
model_name: latest_text_style_embedding_model_senticap.pth
mul_clip_style: true
new_weighted_loss: false
num_classes: 64
num_iterations: 5
num_iterations_clip_style: 1
num_workers: 10
only_clip_styled_clip_loss: true
override: false
plot_only_clustering: false
plot_prob_graphs: true
print_for_debug: false
print_for_debug_redundant: false
repetition_penalty: 2
requires_min_clip_score_val:
  negative: 0.26
  positive: 0.26
requires_min_clip_score_val_neg: 0.35
requires_min_clip_score_val_pos: 0.35
requires_min_fluency_score: 0
requires_min_style_score:
  negative: 0.35
  positive: 0.35
requires_min_style_score_neg: 0.26
requires_min_style_score_pos: 0.26
requires_num_min_clip_score_val:
  negative: 10
  positive: 10
requires_num_min_clip_score_val_neg: 10
requires_num_min_clip_score_val_pos: 10
reset_context_delta: true
results_dir: /home/nlp/tzufar/results
resume: false
reverse_imgs_list: false
run_id: null
run_type: caption
save_config_file: true
scale_noise_txt_cls: 0
seed: 0
sentiment_temperature: 0.01
specific_idxs_to_skip: []
specific_img_idxs_to_test: []
specific_imgs_to_test:
- 13783
- 71738
- 575135
- 31280
- 230701
- 478404
- 544065
- 424842
- 360818
- 155617
- 504732
- 462635
- 89154
- 356002
- 96351
- 35222
- 547258
- 160531
start_loop_clip_style_in_word_num: 1
start_word_loc_heavy_iteration: 2
std_embedding_vectors_negative: 0.020412436
std_embedding_vectors_positive: 0.028914157
std_vec_emb_file: checkpoints/best_models/senticap/senticap_std_class_embedding.p
stepsize: 0.3
stepsize_clip: 0.3
style_img:
  factual: 49
  negative: 51
  positive: 50
style_mul_not_cut: false
style_type: roberta
tags: null
target_seq_length: 30
text_style_scale: 0
text_to_imitate_list:
- positive
th_ce_loss: 10
th_clip_loss: 33
th_style_loss: 33
threshold_sentiment:
  negative: 0.3597
  positive: 0.3597
training_name: 55-cqrot3st-revived-sweep-55
txt_cls_model_path: /home/nlp/tzufar/checkpoints/best_models/senticap/best_senticap_text_style_classification_model.pth
txt_embed_model_paths: /home/nlp/tzufar/checkpoints/best_models/senticap/best_text_style_embedding_model_senticap.pth
undesired_label: neutral
update_ViT: false
use_all_imgs: 0
use_audio_model: true
use_img_path: null
use_single_emoji_style: false
use_style_model: true
use_style_threshold: false
use_text_style_cutting: false
use_text_style_example: false
wandb_mode: online
weight_decay: 1.0e-05
write_debug_tracking_file: false
zerocap_beam_size: 5
zerocap_ce_scale: 0.2
zerocap_clip_scale: 1
zerocap_num_iterations: 5
zerocap_text_style_scale: 0
